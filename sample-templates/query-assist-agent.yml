# This template creates connectors to OpenAI GPT model.
#
# It then creates tools in the Agent Framework to create a query assist agent.
#
# To use:
#  - update the "credential" fields under the create_openai_connector node.
#  - if needed, update region
#
# After provisioning:
#  - returns a workflow ID
#  - use the status API to get the deployed model IDs and agent IDs
#  - use those models and agents to create a chat experience
---
name: Query Assist Agent
description: Create a Query Assist Agent using Bedrock and Sagemaker models
use_case: REGISTER_AGENT
version:
  template: 1.0.0
  compatibility:
  - 2.12.0
  - 3.0.0
# This section defines the provision workflow. Nodes are connected in a graph.
# Either previous_node_inputs or explicit edges can be used to enforce ordering.
workflows:
  provision:
    nodes:
    #
    # SETUP EXTERNAL MODEL
    #
    # Create a connector to an OpenAI model and deploy the model
    - id: create_openai_connector
      type: create_connector
      user_inputs:
        name: OpenAI Chat Connector
        description: The connector to public OpenAI model service for GPT 3.5
        version: '1'
        protocol: http
        parameters:
          endpoint: api.openai.com
          model: gpt-3.5-turbo
        credential:
          openAI_key: 'PUT_YOUR_API_KEY_HERE'
        actions:
        - action_type: predict
          method: POST
          url: https://${parameters.endpoint}/v1/chat/completions
    - id: register_openai_model
      type: register_remote_model
      previous_node_inputs:
        create_openai_connector: connector_id
      user_inputs:
        name: openAI-gpt-3.5-turbo
        deploy: true
    #
    # SETUP PPL AGENT
    #
    # Create a PPLTool
    - id: TransferQuestionToPPLAndExecuteTool
      type: create_tool
      previous_node_inputs:
        register_openai_model: model_id
      user_inputs:
        type: PPLTool
        name: TransferQuestionToPPLAndExecuteTool
        description: 'Use this tool to transfer natural language to generate PPL and
          execute PPL to query inside. Use this tool after you know the index name,
          otherwise, call IndexRoutingTool first. The input parameters are: {index:IndexName,
          question:UserQuestion}'
        parameters:
          response_filter: "$.completion"
          execute: false
          prompt: |
            Below is an instruction that describes a task, paired with the index and corresponding fields that provides further context. Write a response that appropriately completes the request.

            ### Instruction:
            I have an opensearch index with fields in the following. Now I have a question: ${indexInfo.question} Can you help me generate a PPL for that?

            ### Index:
            ${indexInfo.indexName}

            ### Fields:
            ${indexInfo.mappingInfo}

            ### Response:
        include_output_in_agent_response: true
      # Create a flow agent to use the PPLTool
    - id: ppl_agent
      type: register_agent
      previous_node_inputs:
        TransferQuestionToPPLAndExecuteTool: tools
      user_inputs:
        parameters: {}
        app_type: query_assist
        name: PPL agent
        description: this is the PPL agent
        type: flow
    #
    # SETUP RESPONPSE SUMMARY AGENT
    #
    # Create a tool to summarize successful results in natural language
    - id: summarize_success_tool
      type: create_tool
      previous_node_inputs:
        register_openai_model: model_id
      user_inputs:
        type: MLModelTool
        Name: SummarizeSuccessTool
        description: Use this tool to summarize a PPL success response in query assist
        parameters:
          prompt: |2-


            Human: You will be given a search response, summarize it as a concise paragraph while considering the following:
            User's question on index '${parameters.index}': ${parameters.question}
            PPL (Piped Processing Language) query used: ${parameters.query}

            Give some documents to support your point.
            Note that the output could be truncated, summarize what you see. Don't mention about total items returned and don't mention about the fact that output is truncated if you see 'Output is too long, truncated' in the response.

            Skip the introduction; go straight into the summarization.

            Use the following pieces of context to answer the users question.
            If you don't know the answer, just say that you don't know, don't try to make up an answer.
            ----------------
            ${parameters.response}

            Assistant:
          response_filter: "$.completion"
    # Create a flow agent to use the PPLTool
    - id: response_summary_agent
      type: register_agent
      previous_node_inputs:
        summarize_success_tool: tools
      user_inputs:
        parameters: {}
        app_type: query_assist
        name: Response summary agent
        description: this is the summarize success PPL response agent
        type: flow
    #
    # SETUP ERROR AND SUGGESTIONS AGENT
    #
    # Create a tool to summarize error results in natural language
    - id: summarize_error_tool
      type: create_tool
      previous_node_inputs:
        register_openai_model: model_id
      user_inputs:
        type: MLModelTool
        name: SummarizeErrorTool
        description: Use this tool to summarize a PPL error response in query assist
        include_output_in_agent_response: true
        parameters:
          prompt: |2-


            Human: You will be given an API response with errors, summarize it as a concise paragraph. Do not try to answer the user's question.
            If the error cannot be fixed, eg. no such field or function not supported, then give suggestions to rephrase the question.
            It is imperative that you must not give suggestions on how to fix the error or alternative PPL query, or answers to the question.

            Consider the following:
            User's question on index '${parameters.index}': ${parameters.question}
            PPL (Piped Processing Language) query used: ${parameters.query}

            Skip the introduction; go straight into the summarization.

            Use the following pieces of context to answer the users question.
            If you don't know the answer, just say that you don't know, don't try to make up an answer.
            ----------------
            ${parameters.response}

            Assistant:
          response_filter: "$.completion"
    # Create a tool to give suggestions for future questions
    - id: suggestions_tool
      type: create_tool
      previous_node_inputs:
        register_openai_model: model_id
      user_inputs:
        type: MLModelTool
        name: SuggestionsTool
        description: Use this tool to generate possible questions for an index in query
          assist
        include_output_in_agent_response: true
        parameters:
          prompt: |2-


            Human: OpenSearch index: ${parameters.index}

            Recommend 2 or 3 possible questions on this index given the fields below. Only give the questions, do not give descriptions of questions and do not give PPL queries.

            The format for a field is
            ```
            - field_name: field_type (sample field value)
            ```

            Fields:
            ${parameters.fields}

            Put each question in a <question> tag.

            Assistant:
          response_filter: "$.completion"
    # Create a flow agent to summarize the errors and suggest possible questions
    - id: error_summary_agent
      type: register_agent
      previous_node_inputs:
        summarize_error_tool: tools
        suggestions_tool: tools
      user_inputs:
        parameters: {}
        app_type: query_assist
        name: Error summary agent
        description: this is the agent for summarizing PPL error and give suggested questions
        tools_order:
        - summarize_error_tool
        - suggestions_tool
        type: flow
    #
    # WRAP AGENTS IN AGENT TOOLS FOR ROOT AGENT
    #
    - id: ppl_agent_tool
      type: create_tool
      previous_node_inputs:
        ppl_agent: agent_id
      user_inputs:
        description: PPL Agent Tool
        include_output_in_agent_response: true
        type: AgentTool
        parameters:
          max_iteration: '5'
        name: PPLAgentTool
    - id: response_summary_agent_tool
      type: create_tool
      previous_node_inputs:
        response_summary_agent: agent_id
      user_inputs:
        description: Response Summary Agent Tool
        include_output_in_agent_response: true
        type: AgentTool
        parameters:
          max_iteration: '5'
        name: ResponseSummaryPPLAgentTool
    - id: error_summary_agent_tool
      type: create_tool
      previous_node_inputs:
        error_summary_agent: agent_id
      user_inputs:
        description: Error Summary Agent Tool
        include_output_in_agent_response: true
        type: AgentTool
        parameters:
          max_iteration: '5'
        name: ErrorSummaryAgentTool
    # The root agent will use the agent tools
    - id: root_agent
      type: register_agent
      previous_node_inputs:
        ppl_agent_tool: tools
        response_summary_agent_tool: tools
        error_summary_agent_tool: tools
        register_openai_model: model_id
      user_inputs:
        parameters:
          prompt: Answer the question as best you can.
        app_type: chatbot
        name: Root agent
        description: this is the root agent
        tools_order:
        - ppl_agent_tool
        - response_summary_agent_tool
        - error_summary_agent_tool
        memory:
          type: conversation_index
        type: flow
